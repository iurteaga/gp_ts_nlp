\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani2017attention}
\citation{kalyan2021ammus}
\citation{bert}
\citation{roberta}
\citation{Amatriain2023}
\citation{j-Beltagy2019}
\citation{j-Lee2020,j-Alsentzer2019,j-Gu2021}
\citation{kalyan2021ammus}
\citation{j-Gururangan2020}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:submission}{{1}{1}{Introduction}{section.1}{}}
\citation{bert}
\citation{roberta}
\citation{j-kaplan2020}
\citation{patterson2021carbon}
\citation{puvis-de-chavannes-etal-2021-hyperparameter}
\citation{spanbert,ernie}
\citation{roberta}
\citation{b-evolutionaryalgos}
\citation{j-Hennig2012,ip-Hernandez-Lobato2014}
\citation{j-Frazier2018}
\citation{ip-Snoek2012}
\citation{hyperband}
\citation{j-turner2021bayesian}
\citation{hyperband}
\citation{j-Vu2020}
\citation{j-Kang2020}
\citation{ip-Snoek2012,ip-Klein2017,j-turner2021bayesian}
\citation{j-Negoescu2011,j-Calandra2016,ic-Frazier2016,ip-Hernandez-Lobato2017,j-Candelieri2018}
\citation{shahriari2015bayesian,j-Frazier2018}
\citation{j-Maddox2021}
\citation{j-Lai1985}
\citation{b-Lattimore2020}
\citation{j-Slivkins2019}
\citation{j-Gittins1979}
\citation{j-Auer2002}
\citation{j-Lai1987,ip-Kaufmann2012}
\citation{j-Thompson1935}
\citation{j-Lai1987,ip-Kaufmann2012,ip-Agrawal2012,ip-Agrawal2013,ic-Korda2013}
\citation{ip-Lu2017}
\citation{ip-Urteaga2018,j-Urteaga2018}
\citation{ip-Srinivas2010,ip-Gruenewaelder2010}
\citation{ic-Osband2016}
\citation{b-Rasmussen2005}
\citation{ip-Krause2011,ip-Bogunovic2016,ip-Nguyen2020}
\citation{bert}
\citation{bert,roberta}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}{section.2}\protected@file@percent }
\newlabel{sec:background}{{2}{3}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Bayesian optimization and bandits}{3}{subsection.2.1}\protected@file@percent }
\newlabel{ssec:mab}{{2.1}{3}{Bayesian optimization and bandits}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Language model pre-training and the Masked Language Model}{3}{subsection.2.2}\protected@file@percent }
\newlabel{ssec:roberta_pretraining}{{2.2}{3}{Language model pre-training and the Masked Language Model}{subsection.2.2}{}}
\citation{bert}
\citation{roberta}
\citation{roberta}
\newlabel{eq:mlm_loss}{{2}{4}{MLM pre-training}{equation.2.2}{}}
\newlabel{eq:mlm_averagedloss}{{3}{4}{MLM pre-training}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Proposed bandit-based framework}{4}{section.3}\protected@file@percent }
\newlabel{sec:method}{{3}{4}{Proposed bandit-based framework}{section.3}{}}
\citation{b-Rasmussen2005}
\citation{b-Rasmussen2005}
\citation{ip-Titsias2009,ip-Flaxman2015,ip-Pleiss2018}
\citation{j-Russo2018}
\citation{ip-Agrawal2013,ip-Krause2011,ip-Nguyen2020,ip-Srinivas2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}From MLM pre-training to Gaussian process-based regret minimization}{5}{subsection.3.1}\protected@file@percent }
\newlabel{ssec:method_rewards}{{3.1}{5}{From MLM pre-training to Gaussian process-based regret minimization}{subsection.3.1}{}}
\newlabel{eq:reward_mlm_delta}{{4}{5}{From MLM pre-training to Gaussian process-based regret minimization}{equation.3.4}{}}
\newlabel{eq:rewards_gp}{{5}{5}{From MLM pre-training to Gaussian process-based regret minimization}{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}GP-Thompson sampling for TLM pre-training.}{5}{subsection.3.2}\protected@file@percent }
\newlabel{ssec:method_gpts}{{3.2}{5}{GP-Thompson sampling for TLM pre-training}{subsection.3.2}{}}
\citation{roberta}
\citation{fairseq}
\citation{roberta}
\citation{roberta}
\citation{wikitext103}
\citation{c4_realnews}
\citation{bert}
\citation{roberta}
\citation{mimic}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces GP-TS for TLM pre-training\relax }}{6}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:ts_pretrain_hyperparams}{{1}{6}{GP-TS for TLM pre-training\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{6}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Evaluation set-up}{6}{subsection.4.1}\protected@file@percent }
\newlabel{ssec:set_up}{{4.1}{6}{Evaluation set-up}{subsection.4.1}{}}
\citation{kalyan2021ammus}
\citation{robertabase_fairseq}
\citation{glue}
\citation{medMLI}
\citation{mimic}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}GP-TS pre-training of RoBERTa models}{7}{subsection.4.2}\protected@file@percent }
\newlabel{ssec:pretraining}{{4.2}{7}{GP-TS pre-training of RoBERTa models}{subsection.4.2}{}}
\newlabel{fig:pretraining_new_wikic4}{{1a}{8}{\texttt {wiki-c4}.\relax }{figure.caption.7}{}}
\newlabel{sub@fig:pretraining_new_wikic4}{{a}{8}{\texttt {wiki-c4}.\relax }{figure.caption.7}{}}
\newlabel{fig:pretraining_new_mimic}{{1b}{8}{\texttt {mimic}.\relax }{figure.caption.7}{}}
\newlabel{sub@fig:pretraining_new_mimic}{{b}{8}{\texttt {mimic}.\relax }{figure.caption.7}{}}
\newlabel{fig:pretraining_new_ebay}{{1c}{8}{\texttt {e-commerce}.\relax }{figure.caption.7}{}}
\newlabel{sub@fig:pretraining_new_ebay}{{c}{8}{\texttt {e-commerce}.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces MLM validation loss comparison (lower is better) of grid-search and GP-TS based \textit  {from-scratch} pre-trained RoBERTa models, over interactions. \relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig:new_pretraining}{{1}{8}{MLM validation loss comparison (lower is better) of grid-search and GP-TS based \textit {from-scratch} pre-trained RoBERTa models, over interactions. \relax }{figure.caption.7}{}}
\newlabel{fig:continual_pretraining_mimic}{{2a}{8}{\texttt {mimic}.\relax }{figure.caption.8}{}}
\newlabel{sub@fig:continual_pretraining_mimic}{{a}{8}{\texttt {mimic}.\relax }{figure.caption.8}{}}
\newlabel{fig:continual_pretraining_ebay}{{2b}{8}{\texttt {e-commerce}.\relax }{figure.caption.8}{}}
\newlabel{sub@fig:continual_pretraining_ebay}{{b}{8}{\texttt {e-commerce}.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces MLM validation loss comparison (lower is better) of grid-search and GP-TS based \textit  {continually} pre-trained RoBERTa models over interactions. \relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{fig:continual_pretraining}{{2}{8}{MLM validation loss comparison (lower is better) of grid-search and GP-TS based \textit {continually} pre-trained RoBERTa models over interactions. \relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Best MLM loss attained before interactions 20 and 30, when pre-training RoBERTa models continually in the medical domain corpora.\relax }}{8}{table.caption.9}\protected@file@percent }
\newlabel{tab:medical_MLM_medical_robertabase}{{1}{8}{Best MLM loss attained before interactions 20 and 30, when pre-training RoBERTa models continually in the medical domain corpora.\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}GP-TS pre-trained RoBERTa models for downstream fine-tuned tasks}{9}{subsection.4.3}\protected@file@percent }
\newlabel{ssec:downstream}{{4.3}{9}{GP-TS pre-trained RoBERTa models for downstream fine-tuned tasks}{subsection.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Best fine-tuned, downstream task test-set accuracy (higher is better) for continually pre-trained RoBERTa models. The first row corresponds to the fine-tuned performance of the RoBERTa model from which continual pre-training is started.\relax }}{9}{table.caption.10}\protected@file@percent }
\newlabel{tab:ebay_tasks_robertabase}{{2}{9}{Best fine-tuned, downstream task test-set accuracy (higher is better) for continually pre-trained RoBERTa models. The first row corresponds to the fine-tuned performance of the RoBERTa model from which continual pre-training is started.\relax }{table.caption.10}{}}
\citation{roberta}
\citation{patterson2021carbon}
\citation{puvis-de-chavannes-etal-2021-hyperparameter}
\citation{j-Weidinger2021}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{10}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{10}{Conclusion}{section.5}{}}
\bibcite{ip-Agrawal2012}{{1}{2012}{{Agrawal and Goyal}}{{}}}
\bibcite{ip-Agrawal2013}{{2}{2013}{{Agrawal and Goyal}}{{}}}
\bibcite{j-Alsentzer2019}{{3}{2019}{{Alsentzer et~al.}}{{Alsentzer, Murphy, Boag, Weng, Jin, Naumann, and McDermott}}}
\bibcite{Amatriain2023}{{4}{2023}{{Amatriain}}{{}}}
\bibcite{j-Auer2002}{{5}{2002}{{Auer et~al.}}{{Auer, Cesa-Bianchi, and Fischer}}}
\bibcite{j-Beltagy2019}{{6}{2019}{{Beltagy et~al.}}{{Beltagy, Lo, and Cohan}}}
\bibcite{ip-Bogunovic2016}{{7}{2016}{{Bogunovic et~al.}}{{Bogunovic, Scarlett, and Cevher}}}
\bibcite{robertabase_fairseq}{{8}{2022}{{by~Facebook~Research}}{{}}}
\bibcite{j-Calandra2016}{{9}{2016}{{Calandra et~al.}}{{Calandra, Seyfarth, Peters, and Deisenroth}}}
\bibcite{j-Candelieri2018}{{10}{2018}{{Candelieri et~al.}}{{Candelieri, Perego, and Archetti}}}
\bibcite{bert}{{11}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{ip-Flaxman2015}{{12}{2015}{{Flaxman et~al.}}{{Flaxman, Wilson, Neill, Nickisch, and Smola}}}
\bibcite{j-Frazier2018}{{13}{2018}{{Frazier}}{{}}}
\bibcite{ic-Frazier2016}{{14}{2016}{{Frazier and Wang}}{{}}}
\bibcite{gpytorch}{{15}{2018}{{Gardner et~al.}}{{Gardner, Pleiss, Bindel, Weinberger, and Wilson}}}
\bibcite{j-Gittins1979}{{16}{1979}{{Gittins}}{{}}}
\bibcite{ip-Gruenewaelder2010}{{17}{2010}{{Gr\"unew\"alder et~al.}}{{Gr\"unew\"alder, Audibert, Opper, and Shawe-Taylor}}}
\bibcite{j-Gu2021}{{18}{2021}{{Gu et~al.}}{{Gu, Tinn, Cheng, Lucas, Usuyama, Liu, Naumann, Gao, and Poon}}}
\bibcite{j-Gururangan2020}{{19}{2020}{{Gururangan et~al.}}{{Gururangan, Marasović, Swayamdipta, Lo, Beltagy, Downey, and Smith}}}
\bibcite{j-Hennig2012}{{20}{2012}{{Hennig and Schuler}}{{}}}
\bibcite{ip-Hernandez-Lobato2014}{{21}{2014}{{Hern\'{a}ndez-Lobato et~al.}}{{Hern\'{a}ndez-Lobato, Hoffman, and Ghahramani}}}
\bibcite{ip-Hernandez-Lobato2017}{{22}{2017}{{Hern{\'a}ndez-Lobato et~al.}}{{Hern{\'a}ndez-Lobato, Requeima, Pyzer-Knapp, and Aspuru-Guzik}}}
\bibcite{spanbert}{{23}{2020}{{Joshi et~al.}}{{Joshi, Chen, Liu, Weld, Zettlemoyer, and Levy}}}
\bibcite{kalyan2021ammus}{{24}{2021}{{Kalyan et~al.}}{{Kalyan, Rajasekharan, and Sangeetha}}}
\bibcite{j-Kang2020}{{25}{2020}{{Kang et~al.}}{{Kang, Han, and Hwang}}}
\bibcite{j-kaplan2020}{{26}{2020}{{Kaplan et~al.}}{{Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}}}
\bibcite{ip-Kaufmann2012}{{27}{2012}{{Kaufmann et~al.}}{{Kaufmann, Cappe, and Garivier}}}
\bibcite{ip-Klein2017}{{28}{2017}{{Klein et~al.}}{{Klein, Falkner, Bartels, Hennig, and Hutter}}}
\bibcite{ic-Korda2013}{{29}{2013}{{Korda et~al.}}{{Korda, Kaufmann, and Munos}}}
\bibcite{ip-Krause2011}{{30}{2011}{{Krause and Ong}}{{}}}
\bibcite{j-Lai1987}{{31}{1987}{{Lai}}{{}}}
\bibcite{j-Lai1985}{{32}{1985}{{Lai and Robbins}}{{}}}
\bibcite{b-Lattimore2020}{{33}{2020}{{Lattimore and Szepesv{\'a}ri}}{{}}}
\bibcite{j-Lee2020}{{34}{2020}{{Lee et~al.}}{{Lee, Yoon, Kim, Kim, Kim, So, and Kang}}}
\bibcite{hyperband}{{35}{2018}{{Li et~al.}}{{Li, Jamieson, DeSalvo, Rostamizadeh, and Talwalkar}}}
\bibcite{roberta}{{36}{2019}{{Liu et~al.}}{{Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}}}
\bibcite{ip-Lu2017}{{37}{2017}{{Lu and Roy}}{{}}}
\bibcite{j-Maddox2021}{{38}{2021}{{Maddox et~al.}}{{Maddox, Balandat, Wilson, and Bakshy}}}
\bibcite{wikitext103}{{39}{2016}{{Merity et~al.}}{{Merity, Xiong, Bradbury, and Socher}}}
\bibcite{j-Negoescu2011}{{40}{2011}{{Negoescu et~al.}}{{Negoescu, Frazier, and Powell}}}
\bibcite{ip-Nguyen2020}{{41}{2020}{{Nguyen et~al.}}{{Nguyen, Masrani, Brekelmans, Osborne, and Wood}}}
\bibcite{j-Urteaga2018}{{42}{2018}{{{\~n}igo Urteaga and Wiggins}}{{}}}
\bibcite{ic-Osband2016}{{43}{2016}{{Osband et~al.}}{{Osband, Blundell, Pritzel, and Roy}}}
\bibcite{fairseq}{{44}{2019}{{Ott et~al.}}{{Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and Auli}}}
\bibcite{patterson2021carbon}{{45}{2021}{{Patterson et~al.}}{{Patterson, Gonzalez, Le, Liang, Munguia, Rothchild, So, Texier, and Dean}}}
\bibcite{ip-Pleiss2018}{{46}{2018}{{Pleiss et~al.}}{{Pleiss, Gardner, Weinberger, and Wilson}}}
\bibcite{mimic}{{47}{2016}{{Pollard and Johnson}}{{}}}
\bibcite{puvis-de-chavannes-etal-2021-hyperparameter}{{48}{2021}{{Puvis~de Chavannes et~al.}}{{Puvis~de Chavannes, Kongsbak, Rantzau, and Derczynski}}}
\bibcite{b-Rasmussen2005}{{49}{2005}{{Rasmussen and Williams}}{{}}}
\bibcite{j-Russo2018}{{50}{2018}{{Russo et~al.}}{{Russo, Roy, Kazerouni, Osband, and Wen}}}
\bibcite{shahriari2015bayesian}{{51}{2015}{{Shahriari et~al.}}{{Shahriari, Swersky, Wang, Adams, and De~Freitas}}}
\bibcite{medMLI}{{52}{2019}{{Shivade}}{{}}}
\bibcite{j-Slivkins2019}{{53}{2019}{{Slivkins}}{{}}}
\bibcite{ic-Snelson2006}{{54}{2006}{{Snelson and Ghahramani}}{{}}}
\bibcite{ip-Snoek2012}{{55}{2012}{{Snoek et~al.}}{{Snoek, Larochelle, and Adams}}}
\bibcite{ip-Srinivas2010}{{56}{2010}{{Srinivas et~al.}}{{Srinivas, Krause, Kakade, and Seeger}}}
\bibcite{ernie}{{57}{2020}{{Sun et~al.}}{{Sun, Wang, Li, Feng, Tian, Wu, and Wang}}}
\bibcite{j-Thompson1935}{{58}{1935}{{Thompson}}{{}}}
\bibcite{ip-Titsias2009}{{59}{2009}{{Titsias}}{{}}}
\bibcite{j-turner2021bayesian}{{60}{2021}{{Turner et~al.}}{{Turner, Eriksson, McCourt, Kiili, Laaksonen, Xu, and Guyon}}}
\bibcite{ip-Urteaga2018}{{61}{2018}{{Urteaga and Wiggins}}{{}}}
\bibcite{vaswani2017attention}{{62}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{j-Vu2020}{{63}{2020}{{Vu et~al.}}{{Vu, Phung, and Haffari}}}
\bibcite{glue}{{64}{2018}{{Wang et~al.}}{{Wang, Singh, Michael, Hill, Levy, and Bowman}}}
\bibcite{j-Weidinger2021}{{65}{2021}{{Weidinger et~al.}}{{Weidinger, Mellor, Rauh, Griffin, Uesato, Huang, Cheng, Glaese, Balle, Kasirzadeh, et~al.}}}
\bibcite{ip-Wilson2015}{{66}{2015}{{Wilson and Nickisch}}{{}}}
\bibcite{b-evolutionaryalgos}{{67}{2010}{{Yu and Gen}}{{}}}
\bibcite{c4_realnews}{{68}{2019}{{Zellers et~al.}}{{Zellers, Holtzman, Rashkin, Bisk, Farhadi, Roesner, and Choi}}}
\citation{b-Rasmussen2005}
\citation{b-Rasmussen2005}
\citation{b-Rasmussen2005,ip-Pleiss2018}
\citation{ic-Snelson2006,ip-Titsias2009,ip-Wilson2015,ip-Flaxman2015}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix: Gaussian process details}{16}{appendix.A}\protected@file@percent }
\newlabel{asec:GP_details}{{A}{16}{Appendix: Gaussian process details}{appendix.A}{}}
\newlabel{eq:gp_posterior}{{6}{16}{Gaussian process posteriors}{equation.A.6}{}}
\citation{gpytorch}
\@writefile{toc}{\contentsline {section}{\numberline {B}Appendix: Implementation and experimentation details}{17}{appendix.B}\protected@file@percent }
\newlabel{asec:implementation_details}{{B}{17}{Appendix: Implementation and experimentation details}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Gaussian process}{17}{subsection.B.1}\protected@file@percent }
\newlabel{asec:implementation_details_gp}{{B.1}{17}{Gaussian process}{subsection.B.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Gaussian Process prior and hyperparameters.\relax }}{17}{table.caption.17}\protected@file@percent }
\newlabel{tab:gp_prior}{{3}{17}{Gaussian Process prior and hyperparameters.\relax }{table.caption.17}{}}
\citation{fairseq}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}RoBERTa pre-training}{18}{subsection.B.2}\protected@file@percent }
\newlabel{asec:implementation_details_roberta_pretrain}{{B.2}{18}{RoBERTa pre-training}{subsection.B.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces RoBERTa pre-training hyperparameters.\relax }}{18}{table.caption.18}\protected@file@percent }
\newlabel{tab:roberta_pretrain}{{4}{18}{RoBERTa pre-training hyperparameters.\relax }{table.caption.18}{}}
\citation{fairseq}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Summary statistics of the computational cost}{19}{subsection.B.3}\protected@file@percent }
\newlabel{asec:computational_overhead}{{B.3}{19}{Summary statistics of the computational cost}{subsection.B.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Per-interaction execution time of TLM pre-training and GP-TS: average time in seconds, plus-minus the standard deviation.\relax }}{19}{table.caption.19}\protected@file@percent }
\newlabel{tab:pretraining_compcost}{{5}{19}{Per-interaction execution time of TLM pre-training and GP-TS: average time in seconds, plus-minus the standard deviation.\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Summary statistics of the pre-training datasets}{19}{subsection.B.4}\protected@file@percent }
\newlabel{asec:pretraining_dataset_details}{{B.4}{19}{Summary statistics of the pre-training datasets}{subsection.B.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Summary statistics of the pre-training datasets.\relax }}{19}{table.caption.20}\protected@file@percent }
\newlabel{tab:pretraining_dataset_details}{{6}{19}{Summary statistics of the pre-training datasets.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}RoBERTa fine-tuning}{20}{subsection.B.5}\protected@file@percent }
\newlabel{asec:implementation_details_roberta_fine-tune}{{B.5}{20}{RoBERTa fine-tuning}{subsection.B.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces RoBERTa fine-tuning hyperparameters for the e-commerce title classification downstream task.\relax }}{20}{table.caption.21}\protected@file@percent }
\newlabel{tab:roberta_finetune_eclassification}{{7}{20}{RoBERTa fine-tuning hyperparameters for the e-commerce title classification downstream task.\relax }{table.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces RoBERTa fine-tuning hyperparameters for the e-commerce title similarity downstream task.\relax }}{21}{table.caption.22}\protected@file@percent }
\newlabel{tab:roberta_finetune_esimilarity}{{8}{21}{RoBERTa fine-tuning hyperparameters for the e-commerce title similarity downstream task.\relax }{table.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces RoBERTa fine-tuning hyperparameters for the e-commerce title quality downstream task.\relax }}{22}{table.caption.23}\protected@file@percent }
\newlabel{tab:roberta_finetune_equality}{{9}{22}{RoBERTa fine-tuning hyperparameters for the e-commerce title quality downstream task.\relax }{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces RoBERTa fine-tuning hyperparameters for the medical MLI downstream task.\relax }}{23}{table.caption.24}\protected@file@percent }
\newlabel{tab:roberta_finetune_medical}{{10}{23}{RoBERTa fine-tuning hyperparameters for the medical MLI downstream task.\relax }{table.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.6}Summary statistics of the fine-tuning datasets}{24}{subsection.B.6}\protected@file@percent }
\newlabel{asec:finetuning_dataset_details}{{B.6}{24}{Summary statistics of the fine-tuning datasets}{subsection.B.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Summary statistics of the fine-tuning task datasets.\relax }}{24}{table.caption.25}\protected@file@percent }
\newlabel{tab:finetuning_dataset_details}{{11}{24}{Summary statistics of the fine-tuning task datasets.\relax }{table.caption.25}{}}
\gdef \@abspage@last{24}
