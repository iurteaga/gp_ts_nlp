\documentclass{article}
\usepackage[margin=1.0in]{geometry}

%%%% PACKAGES TO USE
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% To work with different affiliations
\usepackage{authblk}

% Math related
\usepackage{amsmath}
% Indicator
\usepackage{dsfont} %works with Type1
% To format itemize/enumerate
\usepackage{enumitem}
% Figures and subfigures
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float} %Stay where told
% Tables
\usepackage{booktabs} % for professional tables
\usepackage{multirow} % to be able to have multiple row expanding cell
\usepackage[table]{xcolor}
\usepackage{pdflscape} % To make them landscape
% Reference column balancing
\usepackage{flushend}
% Algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
% To draw graphs
\usepackage{tikz}
\usetikzlibrary{bayesnet} % Library for bayesian networks

% Bibliography
\usepackage[round,numbers,sort&compress]{natbib}

%%%%%%%% iurteaga definitions %%%%%%
\input{my_definitions}

%% Main document starts here
% Title of the paper
\title{Multi-armed bandits 
for resource efficient, online optimization of \\
language model pre-training: 
the use case of dynamic masking
\vspace*{1ex}
}

% Author information
\input{authors}

\begin{document}
\maketitle

\begin{abstract}
We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs).
TLM pre-training requires high computational resources and introduces many unresolved design choices,
such as selecting its pre-training hyperparameters.
We propose a multi-armed bandit framework for the sequential selection of TLM pre-training hyperparameters,
aimed at optimizing language model performance, in a resource efficient manner.
We design a Thompson sampling algorithm,
with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective,
for its sequential minimization.
Instead of MLM pre-training with fixed masking probabilities,
the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training
by sequentially selecting masking hyperparameters that improve performance.
We empirically demonstrate how GP-TS pre-trains language models efficiently,
\ie it achieves lower MLM loss in fewer epochs, across a variety of settings.
In addition, GP-TS pre-trained TLMs attain competitive downstream performance,
while avoiding expensive hyperparameter grid search.
GP-TS provides an interactive framework for efficient and optimized TLM pre-training that,
by circumventing costly hyperparameter selection,
enables substantial computational savings.
\end{abstract}

\section{Introduction}
\label{sec:submission}
\input{intro}

\section{Background}
\label{sec:background}
\input{background}

\section{Proposed bandit-based framework}
\label{sec:method}
\input{method}

\section{Experiments}
\label{sec:experiments}
\input{experiments}

\section{Conclusion}
\label{sec:conclusion}
\input{conclusion}

\section*{Limitations}
%ACL 2023 requires all submissions to have a section titled ``Limitations'', for discussing the limitations of the paper as a complement to the discussion of strengths in the main text. This section should occur after the conclusion, but before the references. It will not count towards the page limit.
%The discussion of limitations is mandatory. Papers without a limitation section will be desk-rejected without review.
%
%While we are open to different types of limitations, just mentioning that a set of results have been shown for English only probably does not reflect what we expect. 
%Mentioning that the method works mostly for languages with limited morphology, like English, is a much better alternative.
%In addition, limitations such as low scalability to long text, the requirement of large GPU resources, or other things that inspire crucial further investigation are welcome.

There are several limitations to account for in the presented work.
First, the large GPU requirements for the execution and replication of the presented experiments.
Second, the lack of empirical results beyond English-based text,
and how morphologically and syntactically more complex corpora may affect the presented evidence.
Third, our evaluation section compares GP-TS performance to the common hyperparameter grid-search alternative,
yet we acknowledge that other Bayesian optimization techniques used in the machine learning community may provide suitable and competitive alternatives to explore.
In addition, we have not run any hyperparameter tuning beyond MLM dynamic masking, which might improve all studied algorithms' performance.
Finally, our conclusions are limited to RoBERTa models pre-trained via MLM dynamic masking,
and therefore, investigation of how GP-TS generalizes to other TLM pre-training approaches and architectures is lacking.

\section*{Ethics Statement}
%Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

This work raises ethical and societal considerations associated with
the use and biases of pre-collected natural language data,
the energetic and environmental impact of extensive GPU resource usage,
and the downstream applications of language models.
%
We acknowledge the potential implicit biases within the publicly available datasets used.
\Eg \texttt{mimic} reports are limited to the population attended at Beth Israel Deaconess Medical Center,
and may contain implicit biases of health practitioners there.
We have carefully sampled data for the \texttt{e-commerce} dataset to avoid biases over specific products, users and sellers.
%
We are also aware of the rising concerns pertaining to the carbon footprint of large language models~\citep{patterson2021carbon},
and the significant impact hyperparameter selection techniques have on resource utilization and power consumption~\citep{puvis-de-chavannes-etal-2021-hyperparameter}.
Finally, we acknowledge the wide range of established and anticipated risks that language models pose to society~\citep{j-Weidinger2021}.

%\section*{Acknowledgements}
%This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
%EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
%EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
%ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
%ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
%NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
%ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
%NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
%Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
%ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell, 
%ACL 2012 by Maggie Li and Michael White, 
%ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
%ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
%ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
%ACL 2002 by Eugene Charniak and Dekang Lin, 
%and earlier ACL and EACL formats written by several people, including
%John Chen, Henry S. Thompson and Donald Walker.
%Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.
\section*{Acknowledgements}
I\~nigo Urteaga and Moulay-Za\"idane Dra\"idia were partially supported
%by a project funded by eBay's Research and University Partnership for Technology (eRUPT) program.
by funds from eBay's Research and University Partnership for Technology (eRUPT) program.
We also acknowledge computing resources from Columbia Universityâ€™s Shared Research Computing Facility project,
which is supported by NIH Research Facility Improvement Grant 1G20RR030893-01,
and associated funds from the New York State Empire State Development,
Division of Science Technology and Innovation (NYSTAR) Contract C090171.
both awarded April 15, 2010.

% Prior to arxiv submission
%\bibliography{references}
% Select a .bst file for the style
%\bibliographystyle{abbrvnat}
% After compiling, include bbl for ArXiv
\input{main.bbl}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
\clearpage
\appendix
\onecolumn
\input{supplement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
